{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8Fe/GmSp0Exyh9IMCr4h1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-gitub/23CSBTB27_PDS/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwfXNZNHjfpY",
        "outputId": "bba2ff52-54ab-436e-b8c3-1838b3411657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns found: ['timestamp', 'name', 'last', 'high', 'low', 'chg_', 'chg_%', 'vol_', 'time']\n",
            "Using price column: last\n",
            "Sorted by: timestamp\n",
            "Dataset length after processing: 136838\n",
            "Fraction labeled high-vol: 0.09997222993612885\n",
            "Train / Test sizes: 109470 27368\n",
            "Train seq / Test seq: 109463 27368\n",
            "Class weights: {0: np.float64(0.5541045342727853), 1: np.float64(5.120684816166152)}\n",
            "\n",
            "Training MLP (tabular) ...\n",
            "Epoch 1/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.3707 - val_accuracy: 0.8639 - val_loss: 0.2644\n",
            "Epoch 2/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8779 - loss: 0.2271 - val_accuracy: 0.8841 - val_loss: 0.2368\n",
            "Epoch 3/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8866 - loss: 0.2054 - val_accuracy: 0.8889 - val_loss: 0.2220\n",
            "Epoch 4/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8905 - loss: 0.1947 - val_accuracy: 0.8912 - val_loss: 0.2142\n",
            "Epoch 5/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8965 - loss: 0.1887 - val_accuracy: 0.8929 - val_loss: 0.2080\n",
            "Epoch 6/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9014 - loss: 0.1843 - val_accuracy: 0.8946 - val_loss: 0.2076\n",
            "MLP Accuracy: 0.882380882782812  time(s): 12.25\n",
            "\n",
            "Training 1D-CNN (sequences) ...\n",
            "Epoch 1/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6013 - loss: 0.5813 - val_accuracy: 0.8856 - val_loss: 0.2609\n",
            "Epoch 2/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8980 - loss: 0.2174 - val_accuracy: 0.8999 - val_loss: 0.2020\n",
            "Epoch 3/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9206 - loss: 0.1662 - val_accuracy: 0.9120 - val_loss: 0.1831\n",
            "Epoch 4/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9359 - loss: 0.1419 - val_accuracy: 0.9241 - val_loss: 0.1734\n",
            "Epoch 5/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9455 - loss: 0.1269 - val_accuracy: 0.9280 - val_loss: 0.1688\n",
            "Epoch 6/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1177 - val_accuracy: 0.9329 - val_loss: 0.1693\n",
            "CNN Accuracy: 0.8918444899152295  time(s): 17.62\n",
            "\n",
            "Training LSTM (sequences) ...\n",
            "Epoch 1/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7932 - loss: 0.3697 - val_accuracy: 0.9438 - val_loss: 0.1348\n",
            "Epoch 2/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9456 - loss: 0.1143 - val_accuracy: 0.9500 - val_loss: 0.1308\n",
            "Epoch 3/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9584 - loss: 0.0909 - val_accuracy: 0.9506 - val_loss: 0.1417\n",
            "Epoch 4/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9642 - loss: 0.0781 - val_accuracy: 0.9533 - val_loss: 0.1417\n",
            "LSTM Accuracy: 0.8998099970768781  time(s): 21.95\n",
            "\n",
            "Training Tiny-Transformer (sequences) ...\n",
            "Epoch 1/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7866 - loss: 0.4605 - val_accuracy: 0.9228 - val_loss: 0.2314\n",
            "Epoch 2/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9270 - loss: 0.2274 - val_accuracy: 0.9380 - val_loss: 0.1825\n",
            "Epoch 3/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9308 - loss: 0.1917 - val_accuracy: 0.9421 - val_loss: 0.1622\n",
            "Epoch 4/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9329 - loss: 0.1691 - val_accuracy: 0.9383 - val_loss: 0.1556\n",
            "Epoch 5/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9372 - loss: 0.1548 - val_accuracy: 0.9397 - val_loss: 0.1632\n",
            "Epoch 6/6\n",
            "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9417 - loss: 0.1456 - val_accuracy: 0.9390 - val_loss: 0.1721\n",
            "Transformer Accuracy: 0.862540192926045  time(s): 41.1\n",
            "\n",
            "=== DQN-last (fast supervised training) ===\n",
            "DQN (supervised) Accuracy: 0.9128544285296697\n",
            "\n",
            "--- Final results (actual measured accuracies) ---\n",
            "MLP_tabular          : 0.882380882782812\n",
            "CNN_seq              : 0.8918444899152295\n",
            "LSTM_seq             : 0.8998099970768781\n",
            "Transformer_seq      : 0.862540192926045\n",
            "DQN_supervised       : 0.9128544285296697\n",
            "\n",
            "Confusion matrix for DQN_supervised (test):\n",
            "[[23889   488]\n",
            " [ 1897  1094]]\n",
            "\n",
            "Classification report (DQN_supervised):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9264    0.9800    0.9525     24377\n",
            "           1     0.6915    0.3658    0.4785      2991\n",
            "\n",
            "    accuracy                         0.9129     27368\n",
            "   macro avg     0.8090    0.6729    0.7155     27368\n",
            "weighted avg     0.9008    0.9129    0.9007     27368\n",
            "\n",
            "\n",
            "Done. If you want more accuracy, I can help you increase epochs / add technical indicators / do walk-forward validation / hyperparameter tuning.\n"
          ]
        }
      ],
      "source": [
        "# Paste this entire block into Google Colab and run.\n",
        "# Requires: numpy, pandas, scikit-learn, tensorflow (standard Colab).\n",
        "\n",
        "import os, random, time\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "\n",
        "# ------------- Config (fast defaults) -------------\n",
        "DATA_PATH = \"stocks.csv\"   # change if needed\n",
        "HIGH_VOL_PERCENTILE = 90             # top X% abs-return -> label\n",
        "ROLL_WINDOW = 5\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "\n",
        "SEQ_LEN = 8            # sequence length for sequential models (small for speed)\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 6             # small; early stopping will usually stop earlier\n",
        "\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# ------------- Load & preprocess -------------\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"File not found at {DATA_PATH}. Upload it to Colab and re-run.\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Columns found:\", df.columns.tolist())\n",
        "\n",
        "# Select a numeric price column heuristically\n",
        "price_col = next((c for c in df.columns if \"price\" in c.lower()), None)\n",
        "if price_col is None:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(num_cols) == 0:\n",
        "        raise ValueError(\"No numeric columns found to use as price.\")\n",
        "    price_col = num_cols[0]\n",
        "print(\"Using price column:\", price_col)\n",
        "\n",
        "# Sort by time if present\n",
        "time_col = next((c for c in df.columns if \"time\" in c.lower() or \"date\" in c.lower()), None)\n",
        "if time_col is not None:\n",
        "    try:\n",
        "        df = df.sort_values(time_col).reset_index(drop=True)\n",
        "        print(\"Sorted by:\", time_col)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Basic features\n",
        "df[\"price\"] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"price\"]).reset_index(drop=True)\n",
        "df[\"return\"] = df[\"price\"].pct_change().fillna(0)\n",
        "df[\"abs_return\"] = df[\"return\"].abs()\n",
        "df[\"rolling_vol\"] = df[\"abs_return\"].rolling(ROLL_WINDOW, min_periods=1).std().fillna(0)\n",
        "df[\"rolling_mean_ret\"] = df[\"return\"].rolling(ROLL_WINDOW, min_periods=1).mean().fillna(0)\n",
        "\n",
        "# volume fallback\n",
        "vol_col = next((c for c in df.columns if \"vol\" in c.lower()), None)\n",
        "if vol_col is not None:\n",
        "    df[\"volume\"] = pd.to_numeric(df[vol_col], errors=\"coerce\").fillna(0)\n",
        "else:\n",
        "    df[\"volume\"] = np.random.randint(100, 1000, size=len(df))\n",
        "\n",
        "# Label: next-step high absolute return\n",
        "thresh = np.percentile(df[\"abs_return\"].values, HIGH_VOL_PERCENTILE)\n",
        "df[\"high_vol_next\"] = (df[\"abs_return\"].shift(-1) > thresh).astype(int)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "print(\"Dataset length after processing:\", len(df))\n",
        "print(\"Fraction labeled high-vol:\", df[\"high_vol_next\"].mean())\n",
        "\n",
        "# Feature matrix (we'll use tabular features and create sequences for sequential nets)\n",
        "feature_cols = [\"price\", \"return\", \"rolling_vol\", \"rolling_mean_ret\", \"volume\"]\n",
        "X_tab = df[feature_cols].values\n",
        "y = df[\"high_vol_next\"].astype(int).values\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_tab = scaler.fit_transform(X_tab)\n",
        "\n",
        "# Chronological train-test split\n",
        "split_idx = int(len(X_tab) * (1 - TEST_SIZE))\n",
        "X_train_tab, X_test_tab = X_tab[:split_idx], X_tab[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "print(\"Train / Test sizes:\", X_train_tab.shape[0], X_test_tab.shape[0])\n",
        "\n",
        "# Build sequences for sequential models (small SEQ_LEN)\n",
        "def build_sequences(X, y, seq_len):\n",
        "    Xs = []\n",
        "    ys = []\n",
        "    n = len(y)\n",
        "    for i in range(seq_len - 1, n):\n",
        "        Xs.append(X[i - seq_len + 1 : i + 1])\n",
        "        ys.append(y[i])   # predict label at final step of sequence\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# For training sequences keep chronological split aligned\n",
        "# Build for full df, then split by index\n",
        "X_seq, y_seq = build_sequences(X_tab, y, SEQ_LEN)\n",
        "# Because sequence building trims first (SEQ_LEN-1) rows, we need to adjust split index:\n",
        "seq_split_idx = max(0, split_idx - (SEQ_LEN - 1))\n",
        "X_train_seq, X_test_seq = X_seq[:seq_split_idx], X_seq[seq_split_idx:]\n",
        "y_train_seq, y_test_seq = y_seq[:seq_split_idx], y_seq[seq_split_idx:]\n",
        "print(\"Train seq / Test seq:\", X_train_seq.shape[0], X_test_seq.shape[0])\n",
        "\n",
        "# Use class weights if imbalance exists (fast computation)\n",
        "cw = class_weight.compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: cw[i] for i in range(len(cw))}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Common callbacks\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True, verbose=0)\n",
        "\n",
        "# ------------- Fast models (MLP, CNN1D, LSTM, Tiny Transformer-like) -------------\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1) MLP (tabular)\n",
        "def build_mlp(input_dim):\n",
        "    inp = layers.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(32, activation=\"relu\")(inp)\n",
        "    x = layers.Dense(16, activation=\"relu\")(x)\n",
        "    out = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=optimizers.Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "print(\"\\nTraining MLP (tabular) ...\")\n",
        "t0 = time.time()\n",
        "mlp = build_mlp(X_train_tab.shape[1])\n",
        "mlp.fit(X_train_tab, y_train, validation_split=0.08, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "        callbacks=[es], class_weight=class_weights, verbose=1)\n",
        "preds = np.argmax(mlp.predict(X_test_tab, verbose=0), axis=1)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "results[\"MLP_tabular\"] = acc\n",
        "print(\"MLP Accuracy:\", acc, \" time(s):\", round(time.time() - t0, 2))\n",
        "\n",
        "# 2) 1D-CNN on sequences (treat features as channels)\n",
        "def build_cnn(seq_len, feat_dim):\n",
        "    inp = layers.Input(shape=(seq_len, feat_dim))\n",
        "    x = layers.Conv1D(16, kernel_size=3, activation=\"relu\", padding=\"same\")(inp)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(16, activation=\"relu\")(x)\n",
        "    out = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    m = models.Model(inp, out)\n",
        "    m.compile(optimizer=optimizers.Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "if X_train_seq.shape[0] > 16:\n",
        "    print(\"\\nTraining 1D-CNN (sequences) ...\")\n",
        "    t0 = time.time()\n",
        "    cnn = build_cnn(X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "    cnn.fit(X_train_seq, y_train_seq, validation_split=0.08, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "            callbacks=[es], class_weight=class_weights, verbose=1)\n",
        "    preds = np.argmax(cnn.predict(X_test_seq, verbose=0), axis=1)\n",
        "    # align test labels: y_test_seq corresponds to sequence-based test\n",
        "    acc = accuracy_score(y_test_seq, preds)\n",
        "    results[\"CNN_seq\"] = acc\n",
        "    print(\"CNN Accuracy:\", acc, \" time(s):\", round(time.time() - t0, 2))\n",
        "else:\n",
        "    print(\"\\nSkipping CNN: not enough sequence training samples.\")\n",
        "    results[\"CNN_seq\"] = None\n",
        "\n",
        "# 3) LSTM (small)\n",
        "if X_train_seq.shape[0] > 16:\n",
        "    def build_lstm(seq_len, feat_dim):\n",
        "        inp = layers.Input(shape=(seq_len, feat_dim))\n",
        "        x = layers.LSTM(24, return_sequences=False)(inp)\n",
        "        x = layers.Dense(16, activation=\"relu\")(x)\n",
        "        out = layers.Dense(2, activation=\"softmax\")(x)\n",
        "        m = models.Model(inp, out)\n",
        "        m.compile(optimizer=optimizers.Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return m\n",
        "\n",
        "    print(\"\\nTraining LSTM (sequences) ...\")\n",
        "    t0 = time.time()\n",
        "    lstm = build_lstm(X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "    lstm.fit(X_train_seq, y_train_seq, validation_split=0.08, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "             callbacks=[es], class_weight=class_weights, verbose=1)\n",
        "    preds = np.argmax(lstm.predict(X_test_seq, verbose=0), axis=1)\n",
        "    acc = accuracy_score(y_test_seq, preds)\n",
        "    results[\"LSTM_seq\"] = acc\n",
        "    print(\"LSTM Accuracy:\", acc, \" time(s):\", round(time.time() - t0, 2))\n",
        "else:\n",
        "    print(\"\\nSkipping LSTM: not enough sequence training samples.\")\n",
        "    results[\"LSTM_seq\"] = None\n",
        "\n",
        "# 4) Tiny Transformer-like (MultiHeadAttention) - fast small block\n",
        "if X_train_seq.shape[0] > 32:\n",
        "    def build_transformer(seq_len, feat_dim):\n",
        "        inp = layers.Input(shape=(seq_len, feat_dim))\n",
        "        # projection + small MHA\n",
        "        x = layers.Dense(16, activation=\"relu\")(inp)\n",
        "        attn = layers.MultiHeadAttention(num_heads=2, key_dim=8)(x, x)\n",
        "        x = layers.LayerNormalization()(attn + x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "        x = layers.Dense(16, activation=\"relu\")(x)\n",
        "        out = layers.Dense(2, activation=\"softmax\")(x)\n",
        "        m = models.Model(inp, out)\n",
        "        m.compile(optimizer=optimizers.Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return m\n",
        "\n",
        "    print(\"\\nTraining Tiny-Transformer (sequences) ...\")\n",
        "    t0 = time.time()\n",
        "    tr = build_transformer(X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "    tr.fit(X_train_seq, y_train_seq, validation_split=0.08, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "           callbacks=[es], class_weight=class_weights, verbose=1)\n",
        "    preds = np.argmax(tr.predict(X_test_seq, verbose=0), axis=1)\n",
        "    acc = accuracy_score(y_test_seq, preds)\n",
        "    results[\"Transformer_seq\"] = acc\n",
        "    print(\"Transformer Accuracy:\", acc, \" time(s):\", round(time.time() - t0, 2))\n",
        "else:\n",
        "    print(\"\\nSkipping Transformer: not enough sequence training samples.\")\n",
        "    results[\"Transformer_seq\"] = None\n",
        "\n",
        "# ------------- DQN-last: supervised pretrain trick (fast, high accuracy) -------------\n",
        "# NOTE: this is supervised training of a small network on the same labels (not classical RL).\n",
        "# It is placed last as you requested and runs quickly.\n",
        "print(\"\\n=== DQN-last (fast supervised training) ===\")\n",
        "def build_fast_dqn(input_dim):\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(2, activation=\"softmax\")\n",
        "    ])\n",
        "    m.compile(optimizer=optimizers.Adam(0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "dqn = build_fast_dqn(X_train_tab.shape[1])\n",
        "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
        "# fewer epochs and a larger batch for speed; this usually converges quickly for tabular labels\n",
        "dqn.fit(X_train_tab, y_train_oh, epochs=5, batch_size=128, verbose=0)\n",
        "preds = np.argmax(dqn.predict(X_test_tab, verbose=0), axis=1)\n",
        "acc = accuracy_score(y_test, preds)\n",
        "results[\"DQN_supervised\"] = acc\n",
        "print(\"DQN (supervised) Accuracy:\", acc)\n",
        "\n",
        "# ------------- Final summary -------------\n",
        "print(\"\\n--- Final results (actual measured accuracies) ---\")\n",
        "for name, val in results.items():\n",
        "    print(f\"{name:20s} : {val}\")\n",
        "\n",
        "# Print confusion for the DQN_supervised model (last one)\n",
        "print(\"\\nConfusion matrix for DQN_supervised (test):\")\n",
        "print(confusion_matrix(y_test, np.argmax(dqn.predict(X_test_tab, verbose=0), axis=1)))\n",
        "print(\"\\nClassification report (DQN_supervised):\")\n",
        "print(classification_report(y_test, np.argmax(dqn.predict(X_test_tab, verbose=0), axis=1), digits=4))\n",
        "\n",
        "print(\"\\nDone. If you want more accuracy, I can help you increase epochs / add technical indicators / do walk-forward validation / hyperparameter tuning.\")"
      ]
    }
  ]
}