{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzzi82RBAA43qcB72Y1L5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-gitub/23CSBTB27_PDS/blob/main/supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Volatility Prediction Framework\n",
        "# ===============================\n",
        "\n",
        "# Install additional libraries\n",
        "!pip install -q imbalanced-learn xgboost lightgbm catboost\n",
        "\n",
        "# ---- Imports ----\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings, random, pickle\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)\n",
        "\n",
        "# ---- Load dataset ----\n",
        "df = pd.read_csv(\"stocks.csv\")   # <-- in Colab, either upload or put in Drive\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# ---- Detect columns ----\n",
        "timestamp_col = next((c for c in df.columns if \"time\" in c.lower() or \"date\" in c.lower()), None)\n",
        "price_col = next((c for c in df.columns if \"last\" in c.lower() or \"price\" in c.lower()), None)\n",
        "high_col = next((c for c in df.columns if \"high\" in c.lower()), None)\n",
        "low_col = next((c for c in df.columns if \"low\" in c.lower()), None)\n",
        "volume_col = next((c for c in df.columns if \"vol\" in c.lower()), None)\n",
        "\n",
        "print(\"Detected columns:\", timestamp_col, price_col, high_col, low_col, volume_col)\n",
        "\n",
        "# ---- Preprocess ----\n",
        "df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors=\"coerce\")\n",
        "df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
        "df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\").fillna(method=\"ffill\")\n",
        "\n",
        "# ---- Feature engineering ----\n",
        "df[\"return\"] = df[price_col].pct_change().fillna(0)\n",
        "df[\"log_return\"] = np.log(df[price_col]).diff().fillna(0)\n",
        "df[\"hl_range\"] = (df[high_col] - df[low_col]) / df[price_col]\n",
        "\n",
        "df[\"rolling_ret_std_5\"] = df[\"return\"].rolling(5).std().fillna(0)\n",
        "df[\"rolling_ret_std_10\"] = df[\"return\"].rolling(10).std().fillna(0)\n",
        "df[\"rolling_vol_5\"] = df[\"log_return\"].rolling(5).std().fillna(0)\n",
        "df[\"rolling_vol_10\"] = df[\"log_return\"].rolling(10).std().fillna(0)\n",
        "\n",
        "if volume_col:\n",
        "    df[\"volume\"] = pd.to_numeric(df[volume_col], errors=\"coerce\").fillna(0)\n",
        "    df[\"vol_change\"] = df[\"volume\"].pct_change().fillna(0)\n",
        "else:\n",
        "    df[\"volume\"] = 0\n",
        "    df[\"vol_change\"] = 0\n",
        "\n",
        "df[\"hour\"] = df[timestamp_col].dt.hour\n",
        "df[\"minute\"] = df[timestamp_col].dt.minute\n",
        "\n",
        "# ---- Label: future volatility (binary) ----\n",
        "LABEL_WINDOW = 5\n",
        "future_vol = df[\"log_return\"].shift(-1).rolling(LABEL_WINDOW).std()\n",
        "thresh = future_vol.quantile(0.75)\n",
        "df[\"vol_label\"] = (future_vol > thresh).astype(int)\n",
        "\n",
        "df = df.dropna(subset=[\"vol_label\"])\n",
        "print(\"Label distribution:\", df[\"vol_label\"].value_counts(normalize=True))\n",
        "\n",
        "# ---- Features & Target ----\n",
        "features = [\"return\",\"log_return\",\"hl_range\",\"rolling_ret_std_5\",\"rolling_ret_std_10\",\n",
        "            \"rolling_vol_5\",\"rolling_vol_10\",\"volume\",\"vol_change\",\"hour\",\"minute\"]\n",
        "X = df[features].fillna(0)\n",
        "y = df[\"vol_label\"]\n",
        "\n",
        "# ---- Time-based split ----\n",
        "split = int(len(df)*0.7)\n",
        "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
        "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
        "\n",
        "# ---- Balance training data ----\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# ---- Scale ----\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train_bal)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# ---- Models ----\n",
        "models = {\n",
        "    \"LogReg\": LogisticRegression(max_iter=2000, class_weight=\"balanced\"),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"LinearSVC\": LinearSVC(max_iter=5000, class_weight=\"balanced\"),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\"),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, class_weight=\"balanced\"),\n",
        "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=200, class_weight=\"balanced\"),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
        "    \"LightGBM\": lgb.LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"MLP\": MLPClassifier(max_iter=500)\n",
        "}\n",
        "\n",
        "# ---- Train & Evaluate ----\n",
        "results = []\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train_s, y_train_bal)\n",
        "    y_pred = clf.predict(X_test_s)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "    results.append({\"Model\":name,\"Accuracy\":acc,\"Precision\":prec,\"Recall\":rec,\"F1\":f1})\n",
        "\n",
        "# ---- Results summary ----\n",
        "res_df = pd.DataFrame(results).sort_values(\"F1\", ascending=False)\n",
        "print(\"\\nSummary:\\n\", res_df)\n",
        "\n",
        "# ---- Stacking Ensemble (top 3) ----\n",
        "top_models = res_df.head(3)[\"Model\"].tolist()\n",
        "estimators = [(m, models[m]) for m in top_models]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=2000))\n",
        "stack.fit(X_train_s, y_train_bal)\n",
        "y_pred_stack = stack.predict(X_test_s)\n",
        "print(\"\\n=== Stacking Ensemble ===\")\n",
        "print(classification_report(y_test, y_pred_stack, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGZrtBdVLr_F",
        "outputId": "9f87eace-d1db-49d6-bd24-22c6884054b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (136838, 9)\n",
            "             timestamp         name    last    high     low   chg_   chg_%  \\\n",
            "0  2025-09-19 02:00:02       Boeing  215.66  217.40  213.70   1.03  +0.48%   \n",
            "1  2025-09-19 02:00:02      Chevron  158.84  160.29  158.09  -1.25  -0.78%   \n",
            "2  2025-09-19 02:00:02    Citigroup  102.41  102.70  101.69   0.65  +0.64%   \n",
            "3  2025-09-19 02:00:02  Caterpillar  466.96  467.71  448.87  16.30  +3.62%   \n",
            "4  2025-09-19 02:00:02    Microsoft  508.27  513.07  507.66  -1.75  -0.34%   \n",
            "\n",
            "     vol_      time  \n",
            "0   6.32M  15:59:59  \n",
            "1   3.88M  15:59:59  \n",
            "2  11.52M  15:59:59  \n",
            "3    4.3M  15:59:59  \n",
            "4  13.75M  15:59:59  \n",
            "Detected columns: timestamp last high low vol_\n",
            "Label distribution: vol_label\n",
            "0    0.750011\n",
            "1    0.249989\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "=== LogReg ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9288    0.8111    0.8660     30657\n",
            "           1     0.5945    0.8167    0.6881     10395\n",
            "\n",
            "    accuracy                         0.8126     41052\n",
            "   macro avg     0.7617    0.8139    0.7771     41052\n",
            "weighted avg     0.8442    0.8126    0.8210     41052\n",
            "\n",
            "\n",
            "=== KNN ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9268    0.8879    0.9069     30657\n",
            "           1     0.7057    0.7932    0.7469     10395\n",
            "\n",
            "    accuracy                         0.8639     41052\n",
            "   macro avg     0.8163    0.8405    0.8269     41052\n",
            "weighted avg     0.8708    0.8639    0.8664     41052\n",
            "\n",
            "\n",
            "=== LinearSVC ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9257    0.8080    0.8629     30657\n",
            "           1     0.5882    0.8087    0.6810     10395\n",
            "\n",
            "    accuracy                         0.8082     41052\n",
            "   macro avg     0.7569    0.8083    0.7719     41052\n",
            "weighted avg     0.8402    0.8082    0.8168     41052\n",
            "\n",
            "\n",
            "=== DecisionTree ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9169    0.8914    0.9040     30657\n",
            "           1     0.7040    0.7618    0.7318     10395\n",
            "\n",
            "    accuracy                         0.8586     41052\n",
            "   macro avg     0.8105    0.8266    0.8179     41052\n",
            "weighted avg     0.8630    0.8586    0.8604     41052\n",
            "\n",
            "\n",
            "=== RandomForest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9296    0.9279    0.9288     30657\n",
            "           1     0.7886    0.7927    0.7906     10395\n",
            "\n",
            "    accuracy                         0.8937     41052\n",
            "   macro avg     0.8591    0.8603    0.8597     41052\n",
            "weighted avg     0.8939    0.8937    0.8938     41052\n",
            "\n",
            "\n",
            "=== ExtraTrees ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9344    0.9248    0.9296     30657\n",
            "           1     0.7849    0.8086    0.7965     10395\n",
            "\n",
            "    accuracy                         0.8954     41052\n",
            "   macro avg     0.8596    0.8667    0.8631     41052\n",
            "weighted avg     0.8965    0.8954    0.8959     41052\n",
            "\n",
            "\n",
            "=== GradientBoosting ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9346    0.8854    0.9094     30657\n",
            "           1     0.7076    0.8174    0.7585     10395\n",
            "\n",
            "    accuracy                         0.8682     41052\n",
            "   macro avg     0.8211    0.8514    0.8340     41052\n",
            "weighted avg     0.8771    0.8682    0.8712     41052\n",
            "\n",
            "\n",
            "=== AdaBoost ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9480    0.7690    0.8491     30657\n",
            "           1     0.5623    0.8755    0.6848     10395\n",
            "\n",
            "    accuracy                         0.7959     41052\n",
            "   macro avg     0.7552    0.8222    0.7670     41052\n",
            "weighted avg     0.8503    0.7959    0.8075     41052\n",
            "\n",
            "\n",
            "=== XGBoost ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9286    0.9250    0.9268     30657\n",
            "           1     0.7813    0.7904    0.7858     10395\n",
            "\n",
            "    accuracy                         0.8909     41052\n",
            "   macro avg     0.8550    0.8577    0.8563     41052\n",
            "weighted avg     0.8913    0.8909    0.8911     41052\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 71973, number of negative: 71973\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007251 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1816\n",
            "[LightGBM] [Info] Number of data points in the train set: 143946, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "\n",
            "=== LightGBM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9302    0.9207    0.9254     30657\n",
            "           1     0.7729    0.7962    0.7843     10395\n",
            "\n",
            "    accuracy                         0.8891     41052\n",
            "   macro avg     0.8515    0.8584    0.8549     41052\n",
            "weighted avg     0.8903    0.8891    0.8897     41052\n",
            "\n",
            "\n",
            "=== CatBoost ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9304    0.9248    0.9276     30657\n",
            "           1     0.7820    0.7960    0.7889     10395\n",
            "\n",
            "    accuracy                         0.8922     41052\n",
            "   macro avg     0.8562    0.8604    0.8583     41052\n",
            "weighted avg     0.8928    0.8922    0.8925     41052\n",
            "\n",
            "\n",
            "=== GaussianNB ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9033    0.8204    0.8598     30657\n",
            "           1     0.5831    0.7410    0.6527     10395\n",
            "\n",
            "    accuracy                         0.8003     41052\n",
            "   macro avg     0.7432    0.7807    0.7563     41052\n",
            "weighted avg     0.8222    0.8003    0.8074     41052\n",
            "\n",
            "\n",
            "=== MLP ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9165    0.9241    0.9203     30657\n",
            "           1     0.7705    0.7517    0.7610     10395\n",
            "\n",
            "    accuracy                         0.8804     41052\n",
            "   macro avg     0.8435    0.8379    0.8406     41052\n",
            "weighted avg     0.8795    0.8804    0.8799     41052\n",
            "\n",
            "\n",
            "Summary:\n",
            "                Model  Accuracy  Precision    Recall        F1\n",
            "5         ExtraTrees  0.895401   0.784854  0.808562  0.796531\n",
            "4       RandomForest  0.893696   0.788592  0.792689  0.790635\n",
            "10          CatBoost  0.892161   0.782042  0.795960  0.788939\n",
            "8            XGBoost  0.890894   0.781286  0.790380  0.785807\n",
            "9           LightGBM  0.889141   0.772880  0.796152  0.784343\n",
            "12               MLP  0.880420   0.770459  0.751708  0.760968\n",
            "6   GradientBoosting  0.868216   0.707553  0.817412  0.758525\n",
            "1                KNN  0.863880   0.705726  0.793170  0.746897\n",
            "3       DecisionTree  0.858594   0.704036  0.761809  0.731784\n",
            "0             LogReg  0.812555   0.594538  0.816739  0.688146\n",
            "7           AdaBoost  0.795942   0.562346  0.875517  0.684826\n",
            "2          LinearSVC  0.808194   0.588202  0.808658  0.681034\n",
            "11        GaussianNB  0.800278   0.583119  0.741029  0.652658\n",
            "\n",
            "=== Stacking Ensemble ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8932    0.9670    0.9286     30657\n",
            "           1     0.8713    0.6589    0.7503     10395\n",
            "\n",
            "    accuracy                         0.8890     41052\n",
            "   macro avg     0.8822    0.8129    0.8395     41052\n",
            "weighted avg     0.8876    0.8890    0.8835     41052\n",
            "\n"
          ]
        }
      ]
    }
  ]
}