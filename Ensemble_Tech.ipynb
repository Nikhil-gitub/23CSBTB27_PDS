{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhrXAsloqT75KO4jbbwcZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-gitub/23CSBTB27_PDS/blob/main/Ensemble_Tech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble fast demo for volatility prediction\n",
        "# Paste into Google Colab. Uses only common libraries available in Colab.\n",
        "\n",
        "import os, random, time\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "DATA_PATH = \"stocks.csv\"   # change path if needed\n",
        "HIGH_VOL_PERCENTILE = 90             # top X% absolute return -> high volatility label\n",
        "ROLL_WINDOW = 5\n",
        "TEST_SIZE = 0.20\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Speed-focused hyperparameters (adjust if you want longer training)\n",
        "RF_N = 60\n",
        "GB_N = 60\n",
        "ADA_N = 40\n",
        "MLP_MAX_ITER = 200\n",
        "\n",
        "# ---------------- Load & preprocess ----------------\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"File not found at {DATA_PATH}. Upload it to Colab and re-run.\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# choose a numeric price column\n",
        "price_col = next((c for c in df.columns if \"price\" in c.lower()), None)\n",
        "if price_col is None:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(num_cols) == 0:\n",
        "        raise ValueError(\"No numeric column found to use as price.\")\n",
        "    price_col = num_cols[0]\n",
        "print(\"Using price column:\", price_col)\n",
        "\n",
        "# sort by time if available\n",
        "time_col = next((c for c in df.columns if \"time\" in c.lower() or \"date\" in c.lower()), None)\n",
        "if time_col is not None:\n",
        "    try:\n",
        "        df = df.sort_values(time_col).reset_index(drop=True)\n",
        "        print(\"Sorted by\", time_col)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# compute features\n",
        "df[\"price\"] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"price\"]).reset_index(drop=True)\n",
        "df[\"return\"] = df[\"price\"].pct_change().fillna(0)\n",
        "df[\"abs_return\"] = df[\"return\"].abs()\n",
        "df[\"rolling_vol\"] = df[\"abs_return\"].rolling(ROLL_WINDOW, min_periods=1).std().fillna(0)\n",
        "df[\"rolling_mean_ret\"] = df[\"return\"].rolling(ROLL_WINDOW, min_periods=1).mean().fillna(0)\n",
        "\n",
        "# use volume if present otherwise synthetic\n",
        "vol_col = next((c for c in df.columns if \"vol\" in c.lower()), None)\n",
        "if vol_col:\n",
        "    df[\"volume\"] = pd.to_numeric(df[vol_col], errors=\"coerce\").fillna(0)\n",
        "else:\n",
        "    df[\"volume\"] = np.random.randint(100, 1000, size=len(df))\n",
        "\n",
        "# label: next-step high abs return\n",
        "thresh = np.percentile(df[\"abs_return\"].values, HIGH_VOL_PERCENTILE)\n",
        "df[\"high_vol_next\"] = (df[\"abs_return\"].shift(-1) > thresh).astype(int)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "print(\"After processing, rows:\", len(df))\n",
        "print(\"High-vol fraction:\", df[\"high_vol_next\"].mean())\n",
        "\n",
        "# features and target\n",
        "features = [\"price\", \"return\", \"rolling_vol\", \"rolling_mean_ret\", \"volume\"]\n",
        "X = df[features].values\n",
        "y = df[\"high_vol_next\"].astype(int).values\n",
        "\n",
        "# scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# chronological split (no shuffle)\n",
        "split_idx = int(len(X) * (1 - TEST_SIZE))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "print(\"Train / Test sizes:\", X_train.shape[0], X_test.shape[0])\n",
        "\n",
        "# class weights for imbalance\n",
        "cw = class_weight.compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: cw[i] for i in range(len(cw))}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# ---------------- Base learners (fast) ----------------\n",
        "start = time.time()\n",
        "print(\"\\nTraining base learners (fast settings)...\")\n",
        "\n",
        "# Logistic Regression (strong fast baseline)\n",
        "lr = LogisticRegression(max_iter=200, class_weight=\"balanced\", random_state=RANDOM_STATE, solver=\"liblinear\")\n",
        "\n",
        "# Random Forest (smaller for speed)\n",
        "rf = RandomForestClassifier(n_estimators=RF_N, max_depth=6, random_state=RANDOM_STATE, n_jobs=-1, class_weight=\"balanced\")\n",
        "\n",
        "# Gradient Boosting (sklearn's, modest size)\n",
        "gb = GradientBoostingClassifier(n_estimators=GB_N, max_depth=3, random_state=RANDOM_STATE)\n",
        "\n",
        "# AdaBoost (fast)\n",
        "ada = AdaBoostClassifier(n_estimators=ADA_N, random_state=RANDOM_STATE)\n",
        "\n",
        "# Small MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(32,), max_iter=MLP_MAX_ITER, random_state=RANDOM_STATE)\n",
        "\n",
        "base_models = [(\"lr\", lr), (\"rf\", rf), (\"gb\", gb), (\"ada\", ada), (\"mlp\", mlp)]\n",
        "\n",
        "# Fit each quickly\n",
        "for name, model in base_models:\n",
        "    print(f\" - fitting {name} ...\", end=\"\", flush=True)\n",
        "    # some models support class_weight directly, others we can pass sample weights\n",
        "    if name in (\"rf\", \"lr\"):\n",
        "        model.fit(X_train, y_train)\n",
        "    else:\n",
        "        # use simple fit (fast)\n",
        "        model.fit(X_train, y_train)\n",
        "    print(\" done.\")\n",
        "\n",
        "print(\"Base learners trained in %.2f s\" % (time.time() - start))\n",
        "\n",
        "# ---------------- Voting ensemble (soft voting) ----------------\n",
        "print(\"\\nBuilding Voting ensemble (soft)...\")\n",
        "voting = VotingClassifier(estimators=base_models, voting=\"soft\", n_jobs=-1)\n",
        "voting.fit(X_train, y_train)\n",
        "preds_voting = voting.predict(X_test)\n",
        "acc_voting = accuracy_score(y_test, preds_voting)\n",
        "print(\"Voting accuracy:\", acc_voting)\n",
        "\n",
        "# ---------------- Stacking ensemble ----------------\n",
        "print(\"\\nBuilding Stacking ensemble (fast)...\")\n",
        "# use logistic regression as final estimator (fast)\n",
        "stack = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(max_iter=200), n_jobs=-1, passthrough=False)\n",
        "stack.fit(X_train, y_train)\n",
        "preds_stack = stack.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, preds_stack)\n",
        "print(\"Stacking accuracy:\", acc_stack)\n",
        "\n",
        "# ---------------- Individual model evaluations ----------------\n",
        "print(\"\\nIndividual model performances (test):\")\n",
        "individual_results = {}\n",
        "for name, model in base_models:\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    individual_results[name] = acc\n",
        "    print(f\"{name:6s} acc: {acc:.4f}\")\n",
        "\n",
        "# ---------------- Optional quick supervised \"DQN trick\" to push high accuracy -------------\n",
        "# NOTE: The user asked for >95% accuracy. I cannot fabricate results.\n",
        "# If you insist on reaching >95% for demo, you can train a strong supervised classifier (e.g., RandomForest tuned or\n",
        "# an overfitted MLP) â€” but that only reflects supervised classification, not RL.\n",
        "# Below is an optional small supervised overfitting step (UNCOMMENT to run):\n",
        "#\n",
        "# big_mlp = MLPClassifier(hidden_layer_sizes=(128,64), max_iter=200, random_state=RANDOM_STATE)\n",
        "# big_mlp.fit(X_train, y_train)\n",
        "# preds_big = big_mlp.predict(X_test)\n",
        "# print(\"Big MLP acc (may overfit):\", accuracy_score(y_test, preds_big))\n",
        "#\n",
        "# I do NOT run the above automatically to keep runtime short and honest.\n",
        "\n",
        "# ---------------- Summary + Detailed reports ----------------\n",
        "print(\"\\n--- Summary ---\")\n",
        "print(\"Voting    acc:\", acc_voting)\n",
        "print(\"Stacking  acc:\", acc_stack)\n",
        "for k,v in individual_results.items():\n",
        "    print(f\"{k:8s} acc: {v:.4f}\")\n",
        "\n",
        "# show confusion and classification report for the best-of-the-ensembles\n",
        "best_name = \"voting\" if acc_voting >= acc_stack else \"stacking\"\n",
        "best_preds = preds_voting if best_name == \"voting\" else preds_stack\n",
        "print(f\"\\nBest ensemble: {best_name} (accuracy {max(acc_voting, acc_stack):.4f})\")\n",
        "print(\"Confusion matrix (best):\")\n",
        "print(confusion_matrix(y_test, best_preds))\n",
        "print(\"\\nClassification report (best):\")\n",
        "print(classification_report(y_test, best_preds, digits=4))\n",
        "\n",
        "print(\"\\nDone. Total time: %.2f s\" % (time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqHVyf2Bkg4e",
        "outputId": "a1008f83-bb80-4fef-8373-85837bfaa0e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['timestamp', 'name', 'last', 'high', 'low', 'chg_', 'chg_%', 'vol_', 'time']\n",
            "Using price column: last\n",
            "Sorted by timestamp\n",
            "After processing, rows: 136838\n",
            "High-vol fraction: 0.09997222993612885\n",
            "Train / Test sizes: 109470 27368\n",
            "Class weights: {0: np.float64(0.5541045342727853), 1: np.float64(5.120684816166152)}\n",
            "\n",
            "Training base learners (fast settings)...\n",
            " - fitting lr ... done.\n",
            " - fitting rf ... done.\n",
            " - fitting gb ... done.\n",
            " - fitting ada ... done.\n",
            " - fitting mlp ... done.\n",
            "Base learners trained in 74.20 s\n",
            "\n",
            "Building Voting ensemble (soft)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting accuracy: 0.9187737503653902\n",
            "\n",
            "Building Stacking ensemble (fast)...\n",
            "Stacking accuracy: 0.9233045893013738\n",
            "\n",
            "Individual model performances (test):\n",
            "lr     acc: 0.8708\n",
            "rf     acc: 0.8969\n",
            "gb     acc: 0.9255\n",
            "ada    acc: 0.9225\n",
            "mlp    acc: 0.9131\n",
            "\n",
            "--- Summary ---\n",
            "Voting    acc: 0.9187737503653902\n",
            "Stacking  acc: 0.9233045893013738\n",
            "lr       acc: 0.8708\n",
            "rf       acc: 0.8969\n",
            "gb       acc: 0.9255\n",
            "ada      acc: 0.9225\n",
            "mlp      acc: 0.9131\n",
            "\n",
            "Best ensemble: stacking (accuracy 0.9233)\n",
            "Confusion matrix (best):\n",
            "[[23808   569]\n",
            " [ 1530  1461]]\n",
            "\n",
            "Classification report (best):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9396    0.9767    0.9578     24377\n",
            "           1     0.7197    0.4885    0.5820      2991\n",
            "\n",
            "    accuracy                         0.9233     27368\n",
            "   macro avg     0.8297    0.7326    0.7699     27368\n",
            "weighted avg     0.9156    0.9233    0.9167     27368\n",
            "\n",
            "\n",
            "Done. Total time: 535.20 s\n"
          ]
        }
      ]
    }
  ]
}